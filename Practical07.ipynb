{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sahit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sahit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sahit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sahit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the development of algorithms and models to understand, analyze, and generate human language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.',\n",
       " 'It involves the development of algorithms and models to understand, analyze, and generate human language.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_sentence = sent_tokenize(text)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'nlp',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'that',\n",
       " 'focuses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'interaction',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'humans',\n",
       " 'through',\n",
       " 'natural',\n",
       " 'language',\n",
       " '.',\n",
       " 'it',\n",
       " 'involves',\n",
       " 'the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'models',\n",
       " 'to',\n",
       " 'understand',\n",
       " ',',\n",
       " 'analyze',\n",
       " ',',\n",
       " 'and',\n",
       " 'generate',\n",
       " 'human',\n",
       " 'language',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_words = word_tokenize(text.lower())\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', 'involves', 'development', 'algorithms', 'models', 'understand', 'analyze', 'generate', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('English'))\n",
    "symbols = ['(', ')', '.', ',']\n",
    "\n",
    "filtered_text = []\n",
    "\n",
    "for word in tokenized_words:\n",
    "    if word not in stop_words and word not in symbols:\n",
    "        filtered_text.append(word)\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'nlp',\n",
       " 'subfield',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'focus',\n",
       " 'interact',\n",
       " 'comput',\n",
       " 'human',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'involv',\n",
       " 'develop',\n",
       " 'algorithm',\n",
       " 'model',\n",
       " 'understand',\n",
       " 'analyz',\n",
       " 'gener',\n",
       " 'human',\n",
       " 'languag']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed = []\n",
    "\n",
    "for word in filtered_text:\n",
    "    stemmed.append(ps.stem(word))\n",
    "\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'subfield',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'focus',\n",
       " 'interaction',\n",
       " 'computer',\n",
       " 'human',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'involves',\n",
       " 'development',\n",
       " 'algorithm',\n",
       " 'model',\n",
       " 'understand',\n",
       " 'analyze',\n",
       " 'generate',\n",
       " 'human',\n",
       " 'language']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = []\n",
    "\n",
    "for word in filtered_text:\n",
    "    lemmatized.append(wnl.lemmatize(word))\n",
    "\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural : JJ\n",
      "language : NN\n",
      "processing : NN\n",
      "nlp : JJ\n",
      "subfield : JJ\n",
      "artificial : JJ\n",
      "intelligence : NN\n",
      "focuses : VBZ\n",
      "interaction : NN\n",
      "computers : NNS\n",
      "humans : VBZ\n",
      "natural : JJ\n",
      "language : NN\n",
      "involves : VBZ\n",
      "development : NN\n",
      "algorithms : NN\n",
      "models : NNS\n",
      "understand : VBP\n",
      "analyze : JJ\n",
      "generate : NN\n",
      "human : JJ\n",
      "language : NN\n"
     ]
    }
   ],
   "source": [
    "pos_tagging = []\n",
    "pos_tagging.extend(nltk.pos_tag(filtered_text))\n",
    "\n",
    "for word, tag in pos_tagging:\n",
    "    print(f\"{word} : {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.09090909090909091,\n",
       " 'language': 0.13636363636363635,\n",
       " 'processing': 0.045454545454545456,\n",
       " 'nlp': 0.045454545454545456,\n",
       " 'subfield': 0.045454545454545456,\n",
       " 'artificial': 0.045454545454545456,\n",
       " 'intelligence': 0.045454545454545456,\n",
       " 'focuses': 0.045454545454545456,\n",
       " 'interaction': 0.045454545454545456,\n",
       " 'computers': 0.045454545454545456,\n",
       " 'humans': 0.045454545454545456,\n",
       " 'involves': 0.045454545454545456,\n",
       " 'development': 0.045454545454545456,\n",
       " 'algorithms': 0.045454545454545456,\n",
       " 'models': 0.045454545454545456,\n",
       " 'understand': 0.045454545454545456,\n",
       " 'analyze': 0.045454545454545456,\n",
       " 'generate': 0.045454545454545456,\n",
       " 'human': 0.045454545454545456}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF = {word : filtered_text.count(word) / len(filtered_text) for word in filtered_text}\n",
    "TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.4054651081081644,\n",
       " 'language': 0.28768207245178085,\n",
       " 'processing': 0.6931471805599453,\n",
       " 'nlp': 0.6931471805599453,\n",
       " 'subfield': 0.6931471805599453,\n",
       " 'artificial': 0.6931471805599453,\n",
       " 'intelligence': 0.6931471805599453,\n",
       " 'focuses': 0.6931471805599453,\n",
       " 'interaction': 0.6931471805599453,\n",
       " 'computers': 0.6931471805599453,\n",
       " 'humans': 0.6931471805599453,\n",
       " 'involves': 0.6931471805599453,\n",
       " 'development': 0.6931471805599453,\n",
       " 'algorithms': 0.6931471805599453,\n",
       " 'models': 0.6931471805599453,\n",
       " 'understand': 0.6931471805599453,\n",
       " 'analyze': 0.6931471805599453,\n",
       " 'generate': 0.6931471805599453,\n",
       " 'human': 0.6931471805599453}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "no_of_document = 1\n",
    "IDF = {word : math.log(no_of_document / filtered_text.count(word) + 1) for word in filtered_text}\n",
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.036860464373469494,\n",
       " 'language': 0.03922937351615193,\n",
       " 'processing': 0.03150669002545206,\n",
       " 'nlp': 0.03150669002545206,\n",
       " 'subfield': 0.03150669002545206,\n",
       " 'artificial': 0.03150669002545206,\n",
       " 'intelligence': 0.03150669002545206,\n",
       " 'focuses': 0.03150669002545206,\n",
       " 'interaction': 0.03150669002545206,\n",
       " 'computers': 0.03150669002545206,\n",
       " 'humans': 0.03150669002545206,\n",
       " 'involves': 0.03150669002545206,\n",
       " 'development': 0.03150669002545206,\n",
       " 'algorithms': 0.03150669002545206,\n",
       " 'models': 0.03150669002545206,\n",
       " 'understand': 0.03150669002545206,\n",
       " 'analyze': 0.03150669002545206,\n",
       " 'generate': 0.03150669002545206,\n",
       " 'human': 0.03150669002545206}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF = {word : TF[word] * IDF[word] for word in filtered_text}\n",
    "TFIDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
